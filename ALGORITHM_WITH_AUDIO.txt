ALGORITHM: Multi-Modal Food Intake → Nutrition Report + Advice + PDF

INPUTS
  - Food Image (JPG/PNG)
  - Text Input (typed food description)
  - Audio Input (spoken meal description)

OUTPUTS
  - Nutritional Report (JSON: calories + macros + optional micronutrients/notes)
  - Personalized Advice (text)
  - PDF Report (food logs + nutrition + charts)

BEGIN

STEP 1 — USER INTERFACE + AUTHENTICATION
  1.1 Display START
  1.2 Authenticate user (e.g., Clerk)
  1.3 Receive user input: input_type ∈ {IMAGE, TEXT, AUDIO}

STEP 2 — INPUT ROUTING
  IF input_type = IMAGE:
      Go to STEP 3A (Image Preprocessing)
  ELSE IF input_type = TEXT:
      Go to STEP 3B (Text Preprocessing)
  ELSE IF input_type = AUDIO:
      Go to STEP 3C (Audio Preprocessing + Speech-to-Text)
  END IF

STEP 3A — IMAGE PREPROCESSING (IMAGE PATH)
  3A.1 Resize image to 224 × 224
  3A.2 Convert image to RGB
  3A.3 Transform image into tensor
  3A.4 Go to STEP 4A (Image Encoder/Decoder Captioning)

STEP 3B — TEXT PREPROCESSING + ENCODING (TEXT PATH)
  3B.1 Apply token normalization
  3B.2 Perform spell correction
  3B.3 Execute POS tagging (optional, for richer parsing)
  
  3B.4 TEXT ENCODER:
       - Tokenization Layer: WordPiece/BPE tokenization
       - Embedding Layer: Convert tokens to vectors
       - Hidden Layer: 768 dimensions (contextual embeddings)
  
  3B.5 cleaned_text ← preprocessed and encoded text
  3B.6 Go to STEP 4B (Direct Text → Description)

STEP 3C — AUDIO PREPROCESSING + SPEECH-TO-TEXT (AUDIO PATH)
  3C.1 Audio preprocessing:
       - Normalize audio (sample rate to 16kHz, convert to mono)
       - Trim silence, apply denoising (optional)
  
  3C.2 AUDIO ENCODER (Whisper ASR):
       - Mel Spectrogram Conversion: Audio → frequency features
       - Convolutional Layers (2 layers): Feature extraction
       - Transformer Encoder: Self-Attention layers
       - Hidden Layer: 512 dimensions (audio context vector)
       - Decoder: Generate text transcription
  
  3C.3 transcript_raw ← Speech-to-Text output
  
  3C.4 Apply TEXT PREPROCESSING to transcript_raw:
       - Token normalization, spell correction, POS tagging
       - TEXT ENCODER (same as STEP 3B.4)
  
  3C.5 cleaned_text ← preprocessed transcript
  3C.6 Go to STEP 4B (Direct Text → Description)

STEP 4A — IMAGE UNDERSTANDING (ENCODER/DECODER CAPTIONING)
  4A.1 IMAGE ENCODER (ViT-L/14):
       - Patch Embedding Layer: Split image into 16×16 patches
       - Position Embedding: Add spatial information
       - Transformer Blocks (24 layers): Multi-Head Self-Attention
       - Hidden Layer: 1024 dimensions with Layer Normalization
       - Output: 1024-dimensional visual feature vector
  
  4A.2 IMAGE DECODER (BLIP-2 GPT-2):
       - Cross-Attention Layer: Map visual features to text space
       - Transformer Decoder (12 layers): Causal Self-Attention
       - Hidden Layer: 768 dimensions (language model states)
       - Feed-Forward Network: 2048 → 768 dimensions
       - Output Layer: Vocabulary projection
       - caption ← Generated food description
  
  4A.3 Go to STEP 5 (Food Description Fusion)

STEP 4B — TEXT/AUDIO DECODER (DIRECT TEXT PATH)
  4B.1 TEXT PATH DECODER:
       - cleaned_text → direct food description (no generation needed)
  
  4B.2 AUDIO PATH DECODER:
       - transcribed_text → post-processing layer
       - Grammar correction and context refinement
       - Output: food description from audio
  
  4B.3 caption ← cleaned_text (direct user description)
  4B.4 Go to STEP 5 (Food Description Fusion)

STEP 5 — FOOD DESCRIPTION FUSION (PROMPT/CONTEXT MERGE)
  5.1 LangChain Prompt Manager:
      - Input: caption from STEP 4A/4B
      - Context enrichment:
        * Meal time (breakfast/lunch/dinner/snack)
        * Estimated portion sizes
        * Cuisine type
        * User dietary preferences
        * Allergy information
        * Previous meal history
  
  5.2 Prompt Template Construction:
      - Structured prompt format for nutrition estimation
      - Include relevant context tokens
      - combined_description ← Merged and formatted description

STEP 6 — NUTRITIONAL ESTIMATION (RAG + LLM)
  6.1 RAG (Retrieval-Augmented Generation):
      - Vector Store: FAISS or Chroma
      - Embed combined_description into vector space
      - Retrieve top-k similar nutrition references from database
      - Context: food composition data, USDA database, nutrition facts
  
  6.2 LLM INFERENCE (LLaMA-3 8B via Groq):
      - Input: combined_description + retrieved_context
      - Model: LLaMA-3 8B parameters
      - Temperature: 0.3 (for consistent outputs)
      - Max tokens: 1024
      - Prompt engineering: structured nutrition extraction
  
  6.3 OUTPUT PROCESSING:
      - Extract: calories, protein, carbohydrates, fat
      - Optional: fiber, sodium, vitamins, minerals
      - Confidence scores for each estimate
      - nutrition_json ← Structured JSON output

STEP 7 — DAILY TRACKING + STORAGE
  7.1 Update PostgreSQL:
      - Store entry (timestamp, description, nutrition_json)
      - Update daily totals
  7.2 Compare daily totals with user goals

STEP 8 — VISUALIZATION
  8.1 Generate charts (Matplotlib/Plotly):
      - Calories vs target
      - Macronutrient ratios

STEP 9 — ADVICE ENGINE
  9.1 advice_text ← LLaMA + LangChain(goal comparison + nutrition_json + user profile)
  9.2 Perform gap analysis
  9.3 Generate personalized suggestions

STEP 10 — PDF REPORT
  10.1 Compile: food logs + nutrition_json + charts + advice_text
  10.2 pdf_report ← Generate PDF (FPDF or ReportLab)

STEP 11 — USER DOWNLOAD / SHARE
  11.1 Provide options:
      - Save locally
      - Email
      - Cloud storage
  11.2 Display END

END


