BLOCK-4: DECODER — One important formula per block
================================================================================

Notation: d=768, d_ff=2048. ⊕ concat. LN = layer norm.

--------------------------------------------------------------------------------
ONE FLOW: Image path → Text path → Audio path → Fusion → Summary
--------------------------------------------------------------------------------

Visual Feature Projection into GPT-2 Space (BLIP-2 style)
  VW + 1b^⊤ ∈ ℝ^(L×d)

Layer Normalization (used throughout decoder)
  LN(x) = γ(x−μ)/√(σ²+ε) + β

Cross-Attention Layer — Visual Features → Text (BLIP-2 GPT-2 Decoder—Image Path)
  Attn = softmax(QK^⊤/√d)V   ;   out = LN(in+O)

Transformer Decoder x12 — Causal Self-Attention
  M_ij = 0 if j≤i else −∞   ;   out = LN(in+O_self)

Feed-Forward Network — 2048 → 768 dims
  FFN(x) = LN(x + W_2·GELU(xW_1+b_1)+b_2)

Output Layer — Vocabulary Projection and Caption Generation — Food Description Text
  p(y_t) = softmax(xW+b)   ;   y_t = argmax

Text Path - Direct (Cleaned Text → Description)
  e_t = Embed(w_t)+pos_t   ;   f = (1/n)∑e_t

Audio Path-AST Output (Frozen Weights) (Speech → Text → Description)
  S = Mel(a)   ;   p(y_t|·) = softmax(Dec(·))   ;   e_t = Embed(u_t)+pos_t   ;   f = (1/n)∑e_t

Fusion Module Confidence-Weight LongChain Prompt Manager Context Fusion Layer Combined Food Descripter
  P = ⊕c_img⊕c_txt⊕c_asr⊕…   ;   f = (∑w_i f_i)/(∑w_i)

Overall Decoder Mapping (Summary)
  c_img = Dec_img(V), c_txt = Clean(·), c_asr = ASR(a)   ;   P = Merge(·)
